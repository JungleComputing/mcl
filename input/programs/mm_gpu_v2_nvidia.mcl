/*
 * Copyright 2014 Pieter Hijma
 *
 * This file is part of MCL.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 */



module matrixmultiplication


import gpu;
import nvidia;





nvidia void matmul(const int n = 2048, const int m = 2048, const int p = 2048, float[n,m] c, const 
        float[n,p] a, const float[p,m] b) {
    const int nrElsN = 8;
    const int nrBlocksN = n / nrElsN;
    const int nrThreadsM = gpu.hierarchy.blocks.block.threads.max_nr_units;
    const int nrBlocksM = m / nrThreadsM;
    const int nrLoadIters = p / nrThreadsM;
    foreach (const int bi in nrBlocksN blocks) {
        foreach (const int bj in nrBlocksM blocks) {
            shared float[p] l_a as float[nrLoadIters][nrThreadsM] l_a2;
            a as float[n,nrLoadIters][1,nrThreadsM] a2;
            foreach (const int tj in nrThreadsM threads) {
                for (local int ei = 0; ei < nrElsN; ei++) {
                    const int i = bi * nrElsN + ei;
                    for (local int l = 0; l < nrLoadIters; l++) {
                        l_a2[l][tj] = a2[i,l][0,tj];
                    }
                    barrier(shared);
                    const int j = bj * nrThreadsM + tj;
                    local float sum = 0.0;
                    for (local int k = 0; k < p; k++) {
                        sum = sum + l_a[k] * b[k,j];
                    }
                    c[i,j] = sum;
                    barrier(shared);
                }
            }
        }
    }
}



/*

INFO at |project://mcl/input/programs/mm_gpu_v2_nvidia.mcl|(91,1,<11,29>,<11,30>): An example value for n is needed for performance evaluation

INFO at |project://mcl/input/programs/mm_gpu_v2_nvidia.mcl|(1211,1,<32,45>,<32,46>): Data reuse: b[k,j] is accessed for nrBlocksN blocks bi.
INFO at |project://mcl/input/programs/mm_gpu_v2_nvidia.mcl|(937,2,<26,38>,<26,40>): Data reuse: a2[i,l][0,tj] is accessed for nrBlocksM blocks bj.
INFO at |project://mcl/input/programs/mm_gpu_v2_nvidia.mcl|(1211,1,<32,45>,<32,46>): Data reuse: b[k,j] is accessed inside for-loop with index ei but does not depend on it.

INFO at |project://mcl/input/programs/mm_gpu_v2_nvidia.mcl|(710,7,<22,48>,<22,55>): using shared memory: Try to maximize the # blocks per SMP. This depends on the number of threads, amount of shared memory and the number of registers. Now using 1 of 8 blocks
INFO at |project://mcl/input/programs/mm_gpu_v2_nvidia.mcl|(1211,1,<32,45>,<32,46>): b[k,j] is accessed for nrBlocksN blocks bi.
INFO at |project://mcl/input/programs/mm_gpu_v2_nvidia.mcl|(74,6,<11,12>,<11,18>): Memory on_chip is shared by more than one block, carefully consider the usage of memory spaces {"shared"}



*/