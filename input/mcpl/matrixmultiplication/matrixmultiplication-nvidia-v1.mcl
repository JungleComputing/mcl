package matrixmultiplication;

module matrixmultiplication;

export matmul;

import gpu;
import nvidia;


nvidia void matmul(const int n = 2048, const int m = 2048, const int p = 2048, 
		float[n,m] c, const float[n,p] a, const float[p,m] b) {
        
    const int nrElsN = 8;
    const int nrBlocksN = n / nrElsN;
    
    const int nrThreadsM = 32;
    const int nrThreadsM2 = 8;
    const int nrBlocksM = m / nrThreadsM;
    
    const int nrLoadIters = p / nrThreadsM;
    
    a as float[nrBlocksN,nrLoadIters][nrElsN,nrThreadsM] a2;
    c as float[nrBlocksN,nrBlocksM][nrElsN,nrThreadsM] c2;
    
    foreach (const int bi in nrBlocksN blocks) {
        foreach (const int bj in nrBlocksM blocks) {
        
            shared float[nrElsN][nrThreadsM] l_a;
            
            foreach (const int tj in nrThreadsM2 threads) {
            	foreach (const int tjj in nrThreadsM threads) {
		    local float[nrElsN] sums;
	            const int j = bj * nrThreadsM + tj;
                
		    for (local int ei = 0; ei < nrElsN; ei++) {
                	sums[ei] = 0.0;
		    }
                
		    for (local int l = 0; l < nrLoadIters; l++) {
	                for (local int ei = 0; ei < nrElsN; ei++) {
			    l_a[ei][tj] = a2[bi,l][ei,tj];
			}
                    
	                barrier(shared);
	                
	                for (local int k2 = 0; k2 < p/nrLoadIters; k2++) {
			    int k = l * p/nrLoadIters + k2;
	                	
	                    const float bkj = b[k,j];
	                    
	                    for (local int ei = 0; ei < nrElsN; ei++) {
                        	sums[ei] += l_a[ei][k2] * bkj;
			    }
	                }
	                
	                barrier(shared);
	            }
	                
	            for (local int ei = 0; ei < nrElsN; ei++) {
	            	c2[bi,bj][ei,tj] += sums[ei];
		    }
                }
            }
        }
    }
}




/*
INFO at |project://mcl/input/programs/mm_nvidia-v1.mcl|(1167,2,<38,38>,<38,40>): Data reuse: a2[bi,l][ei,tj] is accessed for nrBlocksM blocks bj.

INFO at |project://mcl/input/programs/mm_nvidia-v1.mcl|(759,7,<28,48>,<28,55>): using shared memory: Try to maximize the # blocks per SMP. This depends on the number of threads, amount of shared memory and the number of registers. Now using 4 of 8 blocks

INFO at |project://mcl/input/programs/mm_nvidia-v1.mcl|(1455,1,<46,39>,<46,40>): Data reuse: b[k,j] is accessed for nrBlocksN blocks bi.

INFO at |project://mcl/input/programs/mm_nvidia-v1.mcl|(71,6,<8,12>,<8,18>): Memory on_chip is shared by more than one block, carefully consider the usage of memory spaces {"shared"}

matmulKernel             : avg = 35.1 ms, total =  176 ms, count =         5

#GFLOPS: 488.81 GFLOPS
Effective Bandwidth: 121.37 GB/s
Bandwidth: 1.3337 GB/s

*/

